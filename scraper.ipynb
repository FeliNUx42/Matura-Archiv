{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "Website: [Maturaarbeiten Archiv](https://maturitaetsarbeiten.ch/cms/archiv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoliasearch.search_client import SearchClient\n",
    "import boto3\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from uuid import uuid4\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import sqlite3\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://maturitaetsarbeiten.ch\"\n",
    "PB_URL = \"http://127.0.0.1:8090\"\n",
    "HOME_URL = \"/cms/archiv/a-z-namen-2020.html\"\n",
    "HOME_FILE = \"home.html\"\n",
    "\n",
    "page = requests.get(BASE_URL + HOME_URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "container = soup.find(\"div\", class_=\"uk-child-width-1-1 uk-child-width-1-1@m uk-grid-match\")\n",
    "links = soup.find_all(\"a\", class_=\"uk-link-reset\")\n",
    "\n",
    "conn = sqlite3.connect(\"pb_data/data.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "client = SearchClient.create(\"K4983KEQZ8\", \"0599d99b7c0aa6c92daab423161b4acd\")\n",
    "index = client.init_index(\"arbeiten\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(url):\n",
    "  r = requests.get(url)\n",
    "\n",
    "  key = uuid4().hex\n",
    "  new_url = \"https://matura-archive.s3.eu-central-1.amazonaws.com/\" + key\n",
    "  ending = url.split(\".\")[-1].lower()\n",
    "  \n",
    "  if ending == \"pdf\": type = 'application/pdf'\n",
    "  elif ending == \"jpg\": type = 'image/jpg'\n",
    "  elif ending == \"jpeg\": type = 'image/jpeg'\n",
    "  elif ending == \"png\": type = 'image/png'\n",
    "\n",
    "  extra_args = {'ACL':'public-read', 'ContentType':type}\n",
    "\n",
    "  with open(\"test.pdf\", \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "  s3_client.upload_file(\"test.pdf\", \"matura-archive\", key, ExtraArgs=extra_args)\n",
    "  \n",
    "  return { \"name\": \"Anhang\", \"key\": key, \"url\": new_url, \"type\":type, \"size\": len(r.content) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entry(page):\n",
    "  for i, f in enumerate(page[\"fach\"]):\n",
    "    r = requests.post(PB_URL+\"/api/collections/fach/records\", json={\"name\":f})\n",
    "    page[\"fach\"][i] = r.json()[\"id\"]\n",
    "  \n",
    "  for i, b in enumerate(page[\"betreuer\"]):\n",
    "    r = requests.post(PB_URL+\"/api/collections/betreuer/records\", json={\"name\":b})\n",
    "    page[\"betreuer\"][i] = r.json()[\"id\"]\n",
    "  \n",
    "  for i, l in enumerate(page[\"links\"]):\n",
    "    r = requests.post(PB_URL+\"/api/collections/link/records\", json={\"url\":l})\n",
    "    page[\"links\"][i] = r.json()[\"id\"]\n",
    "  \n",
    "  for i, l in enumerate(page[\"anhang\"]):\n",
    "    file = upload_file(l)\n",
    "    r = requests.post(PB_URL+\"/api/collections/datei/records\", json=file)\n",
    "    page[\"anhang\"][i] = r.json()[\"id\"]\n",
    "  \n",
    "  file = upload_file(page[\"arbeit\"])\n",
    "  r = requests.post(PB_URL+\"/api/collections/datei/records\", json=file)\n",
    "  page[\"arbeit\"] = r.json()[\"id\"]\n",
    "  \n",
    "  r = requests.post(PB_URL+\"/api/collections/archiv/records\", json=page)\n",
    "\n",
    "  print(200 if r.status_code == 200 else r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fach(f):\n",
    "  f = f.text.strip()\n",
    "  f = f.split(\": \")[1]\n",
    "  f = re.sub(\"\\(.+\\)\", \"\", f)\n",
    "  f = re.sub(\" ?/ ?$\", \"\", f)\n",
    "  f = re.sub(\", \", \" / \", f)\n",
    "  f = re.sub(\" und \", \" / \", f)\n",
    "  f = re.split(\" ?/ ?\", f)\n",
    "\n",
    "  return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(url, BASE_URL, i):\n",
    "  page = requests.get(BASE_URL + url)\n",
    "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "  data1 = soup.find(\"div\", class_=\"uk-tile-primary uk-tile\")\n",
    "  jahr, name = data1.find_all(\"h2\")\n",
    "  titel = data1.find(\"h1\")\n",
    "  untertitel = data1.find(\"h3\")\n",
    "\n",
    "  data2 = soup.find(\"div\", class_=\"tm-grid-expand uk-grid-collapse\")\n",
    "  *betreuer, schule = data2.find_all(\"div\", class_=\"uk-panel uk-margin\")\n",
    "  \n",
    "  data3 = soup.find(\"div\", class_=\"uk-tile-default uk-tile\")\n",
    "  datei, *sonst = data3.find_all(\"a\")\n",
    "  fach = data3.find(\"h6\")\n",
    "  intro = data3.find(\"div\", class_=\"uk-panel uk-text-lead uk-margin\") or data3.find(\"div\", class_=\"uk-panel uk-text-lead uk-text-emphasis uk-margin\")\n",
    "  abstract = data3.find(\"div\", class_=\"uk-panel uk-margin\")\n",
    "\n",
    "  link_div = soup.find_all(\"div\", class_=\"uk-margin uk-text-center\") or []\n",
    "  for div in link_div:\n",
    "    sonst += div.find_all(\"a\")\n",
    "  \n",
    "  anhang = [BASE_URL + l[\"href\"] for l in sonst if l[\"href\"].startswith(\"/cms\")]\n",
    "  links = [l[\"href\"] for l in sonst if not l[\"href\"].startswith(\"/cms\")]\n",
    "\n",
    "  return {\n",
    "    \"jahr\": int(jahr.text.strip()),\n",
    "    \"schule\": schule.text.strip().replace(\"Schule: \", \"\"),\n",
    "    \"sprache\": \"Deutsch\",\n",
    "    \"farbe\": [\"r\", \"g\", \"b\"][i % 3],\n",
    "    \"name\": name.text.strip(),\n",
    "    \"titel\": titel.text.strip(),\n",
    "    \"untertitel\": getattr(untertitel, \"text\", \"\").strip(),\n",
    "    \"betreuer\": [b.text.strip().split(\": \")[1] for b in betreuer],\n",
    "    \"fach\": parse_fach(fach),\n",
    "    \"intro\": getattr(intro, \"text\", \"\").strip(),\n",
    "    \"abstract\": abstract.text.strip(),\n",
    "    \"arbeit\": BASE_URL + datei[\"href\"],\n",
    "    \"anhang\": anhang,\n",
    "    \"links\": links,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hit(hit):\n",
    "  expand = hit[\"expand\"]\n",
    "  id = hit[\"id\"]\n",
    "\n",
    "  excl = [\"collectionId\", \"collectionName\", \"updated\", \"expand\", \"id\"]\n",
    "  hit = dict((k,v) for k,v in hit.items() if not k in excl)\n",
    "  excl.append(\"created\")\n",
    "\n",
    "  del hit[\"anhang\"]\n",
    "  del hit[\"arbeit\"]\n",
    "  del hit[\"links\"]\n",
    "  \n",
    "  for i, obj in enumerate(expand.get(\"betreuer\", [])):\n",
    "    hit[\"betreuer\"][i] = dict((k,v) for k,v in obj.items() if not k in excl)\n",
    "  for i, obj in enumerate(expand.get(\"fach\", [])):\n",
    "    hit[\"fach\"][i] = dict((k,v) for k,v in obj.items() if not k in excl)\n",
    "  \n",
    "  hit[\"objectID\"] = id\n",
    "\n",
    "  return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n",
      "200\n",
      "1\n",
      "200\n",
      "2\n",
      "200\n",
      "3\n",
      "200\n",
      "4\n",
      "200\n",
      "5\n",
      "200\n",
      "6\n",
      "200\n",
      "7\n",
      "200\n",
      "8\n",
      "200\n",
      "9\n",
      "200\n",
      "10\n",
      "200\n",
      "11\n",
      "200\n",
      "12\n",
      "200\n",
      "13\n",
      "200\n",
      "14\n",
      "200\n",
      "15\n",
      "200\n",
      "16\n",
      "200\n",
      "17\n",
      "200\n",
      "18\n",
      "200\n",
      "19\n",
      "200\n",
      "20\n",
      "200\n",
      "21\n",
      "200\n",
      "22\n",
      "200\n",
      "23\n",
      "200\n",
      "24\n",
      "200\n",
      "25\n",
      "200\n",
      "26\n",
      "200\n",
      "27\n",
      "200\n",
      "28\n",
      "200\n",
      "29\n",
      "200\n",
      "30\n",
      "200\n",
      "31\n",
      "200\n",
      "32\n",
      "200\n",
      "33\n",
      "200\n",
      "34\n",
      "200\n",
      "35\n",
      "200\n",
      "36\n",
      "200\n",
      "37\n",
      "200\n",
      "38\n",
      "200\n",
      "39\n",
      "200\n",
      "40\n",
      "200\n",
      "41\n",
      "200\n",
      "42\n",
      "200\n",
      "43\n",
      "200\n",
      "44\n",
      "200\n",
      "45\n",
      "200\n",
      "46\n",
      "200\n",
      "47\n",
      "200\n",
      "48\n",
      "200\n",
      "49\n",
      "200\n",
      "50\n",
      "200\n",
      "51\n",
      "200\n",
      "52\n",
      "200\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "for n, i in enumerate(links):\n",
    "  try:\n",
    "    page = parse_page(i[\"href\"], BASE_URL, n)\n",
    "    create_entry(page)\n",
    "    print(n)\n",
    "  except Exception as err:\n",
    "    print(f\"Error {n}:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<algoliasearch.responses.IndexingResponse at 0x7fcc70104d30>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = requests.get(PB_URL+\"/api/collections/archiv/records\", params={\"page\":2, \"perPage\":100, \"sort\":\"created,id\", \"expand\":\"betreuer,fach,links,arbeit,anhang\"}).json()\n",
    "\n",
    "hits = list(map(parse_hit, hits[\"items\"]))\n",
    "\n",
    "index.save_objects(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
